{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mapping = pd.read_excel('/mnt/SSD5/yipeng/BIDS_combined/combined/Draft_metadata_all_subjects.xlsx', sheet_name='Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = mapping[['Database No.', 'Paper No.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['Paper No.'] = mapping['Paper No.'].str.replace('_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCLA31\n",
      "0.0\n",
      "UCLA28\n",
      "0.0\n",
      "UCLA03\n",
      "0.0\n",
      "UCLA41\n",
      "0.0\n",
      "UCLA27\n",
      "0.0\n",
      "UCLA15\n",
      "0.0\n",
      "UCLA01\n",
      "0.0\n",
      "UCLA05\n",
      "0.0\n",
      "UCLA06\n",
      "0.0\n",
      "UCLA23\n",
      "0.0\n",
      "UCLA24\n",
      "0.0\n",
      "UCLA34\n",
      "0.0\n",
      "UCLA19\n",
      "0.0\n",
      "UCLA13\n",
      "0.0\n",
      "UCLA30\n",
      "0.0\n",
      "UCLA12\n",
      "0.0\n",
      "UCLA09\n",
      "0.0\n",
      "UCLA48\n",
      "0.0\n",
      "UCLA07\n",
      "0.0\n",
      "UCLA26\n",
      "0.0\n",
      "UCLA04\n",
      "0.0\n",
      "UCLA38\n",
      "0.0\n",
      "UCLA10\n",
      "0.0\n",
      "UCLA20\n",
      "0.0\n",
      "UCLA21\n",
      "0.0\n",
      "UCLA02\n",
      "0.0\n",
      "UCLA14\n",
      "0.0\n",
      "UCLA33\n",
      "0.0\n",
      "UCLA16\n",
      "0.0\n",
      "UCLA08\n",
      "0.0\n",
      "UCLA17\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "folder1 = \"/mnt/SSD5/yipeng/VAE/data_new/\"\n",
    "folder2 = \"/mnt/SSD5/yipeng/VAE/data/\"\n",
    "dataset = \"detroit\"\n",
    "pt_names1 = os.listdir(os.path.join(folder1, dataset))\n",
    "pt_names2 = os.listdir(os.path.join(folder2, dataset))\n",
    "for i in range(len(pt_names1)):\n",
    "    pt_name1 = pt_names1[i]\n",
    "    print(pt_name1.split(\"-\")[1].replace(\"_\",\"\"))\n",
    "    pt_name2 = mapping.loc[mapping['Paper No.'] == pt_name1.split(\"-\")[1], 'Database No.'].values[0]\n",
    "    feature_fn1 = os.path.join(folder1, dataset, pt_name1, \"feature.npz\")\n",
    "    feature_fn2 = os.path.join(folder2, dataset, pt_name2, \"feature.npz\")\n",
    "    \n",
    "    feature1 = np.load(feature_fn1)[\"feature\"]\n",
    "    feature2 = np.load(feature_fn2)[\"feature\"]\n",
    "    # feature 1 and feature 2 are both (n, 64, 64)\n",
    "    # I want to calculate if they are the same\n",
    "    # lets output one value for each feature\n",
    "    print(np.mean(np.abs(feature1 - feature2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCLA18\n",
      "0.0\n",
      "UCLA36\n",
      "0.0\n",
      "UCLA44\n",
      "0.0\n",
      "UCLA49\n",
      "0.0\n",
      "UCLA11\n",
      "0.0\n",
      "UCLA39\n",
      "0.0\n",
      "UCLA25\n",
      "0.0\n",
      "UCLA32\n",
      "0.0\n",
      "UCLA45\n",
      "0.0\n",
      "UCLA50\n",
      "0.0\n",
      "UCLA35\n",
      "0.0\n",
      "UCLA40\n",
      "0.0\n",
      "UCLA43\n",
      "0.0\n",
      "UCLA42\n",
      "0.0\n",
      "UCLA47\n",
      "0.0\n",
      "UCLA46\n",
      "0.0\n",
      "UCLA29\n",
      "0.0\n",
      "UCLA37\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "folder1 = \"/mnt/SSD5/yipeng/VAE/data_new/\"\n",
    "folder2 = \"/mnt/SSD5/yipeng/VAE/data/\"\n",
    "dataset = \"seeg\"\n",
    "pt_names1 = os.listdir(os.path.join(folder1, dataset))\n",
    "pt_names2 = os.listdir(os.path.join(folder2, dataset))\n",
    "for i in range(len(pt_names1)):\n",
    "    pt_name1 = pt_names1[i]\n",
    "    print(pt_name1.split(\"-\")[1].replace(\"_\",\"\"))\n",
    "    pt_name2 = mapping.loc[mapping['Paper No.'] == pt_name1.split(\"-\")[1], 'Database No.'].values[0]\n",
    "    feature_fn1 = os.path.join(folder1, dataset, pt_name1, \"feature.npz\")\n",
    "    feature_fn2 = os.path.join(folder2, dataset, pt_name2, \"feature.npz\")\n",
    "    \n",
    "    feature1 = np.load(feature_fn1)[\"feature\"]\n",
    "    feature2 = np.load(feature_fn2)[\"feature\"]\n",
    "    # feature 1 and feature 2 are both (n, 64, 64)\n",
    "    # I want to calculate if they are the same\n",
    "    # lets output one value for each feature\n",
    "    print(np.sum(np.abs(feature1 - feature2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /mnt/SSD2/Atsuro_sz_data/Ave_new/AN2019_A_ave.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 11451999  =      0.000 ...  5725.999 secs...\n",
      "Extracting EDF parameters from /mnt/SSD2/Atsuro_sz_data/Ave_new/AN2019_B_ave.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 11451999  =      0.000 ...  5725.999 secs...\n",
      "Overwriting existing file.\n"
     ]
    }
   ],
   "source": [
    "fn = \"/mnt/SSD2/Atsuro_sz_data/Ave_new/AN2019_A_ave.edf\"\n",
    "#raw1 = mne.io.read_raw_edf(fn, preload=True)\n",
    "fn2 = \"/mnt/SSD2/Atsuro_sz_data/Ave_new/AN2019_B_ave.edf\"\n",
    "#raw2 = mne.io.read_raw_edf(fn2, preload=True)\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "def merge_edf(file1, file2, output_file):\n",
    "    # Read the two EDF files\n",
    "    raw1 = mne.io.read_raw_edf(file1, preload=True)\n",
    "    raw2 = mne.io.read_raw_edf(file2, preload=True)\n",
    "    raw1.add_channels([raw2])\n",
    "\n",
    "    # Save the combined raw data to a new EDF file\n",
    "    raw1.export(output_file, fmt='edf', overwrite=True)\n",
    "\n",
    "# Example usage\n",
    "merge_edf(fn, fn2, 'merged.edf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00013255,  0.0001842 ,  0.00022797, ..., -0.03432889,\n",
       "       -0.03433991, -0.03432889])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G1',\n",
       " 'G2',\n",
       " 'G3',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'G6',\n",
       " 'G7',\n",
       " 'G8',\n",
       " 'G9',\n",
       " 'G10',\n",
       " 'G11',\n",
       " 'G12',\n",
       " 'G13',\n",
       " 'G15',\n",
       " 'G16',\n",
       " 'G17',\n",
       " 'G18',\n",
       " 'G19',\n",
       " 'G21',\n",
       " 'G22',\n",
       " 'G23',\n",
       " 'G24',\n",
       " 'G26',\n",
       " 'G27',\n",
       " 'G28',\n",
       " 'G31',\n",
       " 'G32',\n",
       " 'G33',\n",
       " 'G34',\n",
       " 'G37',\n",
       " 'G38',\n",
       " 'G39',\n",
       " 'G40',\n",
       " 'G41',\n",
       " 'G42',\n",
       " 'G44',\n",
       " 'G47',\n",
       " 'G49',\n",
       " 'G50',\n",
       " 'G51',\n",
       " 'G52',\n",
       " 'G53',\n",
       " 'G54',\n",
       " 'G55',\n",
       " 'G56',\n",
       " 'G57',\n",
       " 'G58',\n",
       " 'G59',\n",
       " 'G60',\n",
       " 'G61',\n",
       " 'G62',\n",
       " 'G63',\n",
       " 'G64']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.info['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /mnt/SSD5/yipeng/BIDS_combined/combined/sub-UCLA30/ses-01/ieeg/sub-UCLA30_ses-01_task-sleep_ieeg.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 11451999  =      0.000 ...  5725.999 secs...\n",
      "['G1', 'G2', 'G3', 'G4', 'G5', 'G6', 'G7', 'G8', 'G9', 'G10', 'G11', 'G12', 'G13', 'G15', 'G16', 'G17', 'G18', 'G19', 'G21', 'G22', 'G23', 'G24', 'G26', 'G27', 'G28', 'G31', 'G32', 'G33', 'G34', 'G37', 'G38', 'G39', 'G40', 'G41', 'G42', 'G44', 'G47', 'G49', 'G50', 'G51', 'G52', 'G53', 'G54', 'G55', 'G56', 'G57', 'G58', 'G59', 'G60', 'G61', 'G62', 'G63', 'G64', 'DAM1', 'DAM2', 'DAM3', 'DAM4', 'DAM5', 'DAM6', 'DM1', 'DM2', 'DM3', 'DM4', 'DM5', 'DM6', 'DP1', 'DP2', 'DP3', 'DP4', 'DP5', 'DP6', 'DL1', 'DL2', 'DL3', 'DL4', 'DL5', 'DL6', 'ST1', 'ST2', 'ST3', 'ST4', 'ST5', 'ST6', 'ST7', 'ST8', 'SPO1', 'SPO2', 'SPO3', 'SPO4', 'SPO5', 'SPO6', 'SPO7', 'SPO8']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00083879,  0.00081109,  0.00077415, ..., -0.02163528,\n",
       "       -0.02163528, -0.02163528])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "fn1 = \"/mnt/SSD5/yipeng/BIDS_combined/combined/sub-UCLA30/ses-01/ieeg/sub-UCLA30_ses-01_task-sleep_ieeg.edf\"\n",
    "raw1 = mne.io.read_raw_edf(fn1, preload=True)\n",
    "print(raw1.info['ch_names'])\n",
    "a = raw1.get_data()\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /mnt/SSD2/Atsuro_sz_data/Ave_new/AN2019_B_ave.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 11451999  =      0.000 ...  5725.999 secs...\n",
      "['DAM1', 'DAM2', 'DAM3', 'DAM4', 'DAM5', 'DAM6', 'DM1', 'DM2', 'DM3', 'DM4', 'DM5', 'DM6', 'DP1', 'DP2', 'DP3', 'DP4', 'DP5', 'DP6', 'DL1', 'DL2', 'DL3', 'DL4', 'DL5', 'DL6', 'ST1', 'ST2', 'ST3', 'ST4', 'ST5', 'ST6', 'ST7', 'ST8', 'SPO1', 'SPO2', 'SPO3', 'SPO4', 'SPO5', 'SPO6', 'SPO7', 'SPO8']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00083954,  0.00081115,  0.00077483, ..., -0.0216356 ,\n",
       "       -0.0216356 , -0.0216356 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn2= \"/mnt/SSD2/Atsuro_sz_data/Ave_new/AN2019_B_ave.edf\"\n",
    "raw2 = mne.io.read_raw_edf(fn2, preload=True)\n",
    "print(raw2.info['ch_names'])\n",
    "b = raw2.get_data()\n",
    "b[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HFO-Classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
